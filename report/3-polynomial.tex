\chapter{Polynomial Oversampling and Recovery}

Standard algorithms for polynomial interpolation and evaluation, such as Newton interpolation and Horner's method, require $O(n^2)$ time.
Efficient $O(n \log n)$ algorithms are used instead, based on FFT-like transforms introduced in \cite{novel-poly}.

\section{Polynomial Basis}

The polynomial basis $\mathbb{X} = \{X_0, \ldots, X_{2^{64} - 1}\}$ admits transforms $\Psi_h^l$ and $(\Psi_h^l)^{-1}$ which convert between values at $h$ contiguous points with an arbitrary offset $l$ and coefficients in $\mathbb{X}$, with $h$ a power of two.

To encode a $RS(n, k)$ code, the data polynomial coefficients are obtained by applying $(\Psi_h^0)^{-1}$ to the input values, then additional values are obtained using $\Psi_h^l$ $\frac{n}{k}$ times at offsets $l = k, 2k, \ldots, n - k$.

The basis polynomials $X_i$ are defined as the products of polynomials $\hat{W}_j$ corresponding to the bits of the index $i$:
\begin{equation}X_i = \prod_{j \in \text{bits}(i)} \hat{W}_j\end{equation}

$\hat{W}_i = \frac{W_i}{W_i(2^{i})}$ is a normalized vanishing polynomial of degree $2^{i}$, which vanishes (i.e. evaluates to zero) at the points $\omega_0, \omega_1, \ldots, \omega_{2^{i} - 1}$, and evaluates to $1$ at $\omega_{2^{i}}$.
\begin{equation}\hat{W}_i(x) = \frac{W_i(x)}{W_i(2^{i})} = \frac{\prod_{j = 0}^{2^i - 1} (x - \omega_j)}{\prod_{j = 0}^{2^i - 1} (\omega_{2^i} - \omega_j)}\end{equation}

$\hat{W}_i$ has degree $2^{i}$, as it is the product of $2^{i}$ degree one factors divided by a constant. Therefore, $X_i$ has degree $i$, since it the product of $W_j$ corresponding to the bits set in $i$.
Since $\mathbb{X}$ contains $2^{64}$ polynomials with all degrees from $0$ to $2^{64} - 1$, it automatically is a valid basis for representing polynomials of degree up to $2^{64} - 1$.

All $W_i$ are linearized polynomials, which means they only have non-zero coefficients at power-of-two indices and are additive:
\begin{equation}W_i(x + y) = W_i(x) + W_i(y)\end{equation}

Note that the standard monomial basis $\{1, x, x^2, \ldots, x^{2^{64} - 1}\}$ could also be defined in a similar way, with $\hat{W}_i = X^{2^{i}}$, but that would not be sufficient to allow FFT-like fast transforms.

The definition of $\hat{W}_i$ as normalized vanishing polynomials is critical for simplifying equation \ref{eq:1} into \ref{eq:1-simplified}, without which the transforms would not be $O(n \log n)$.

\section{Forward and Inverse Transforms}

Let $D_h$ be the data polynomial with $h$ coefficients $d_0, d_1, \ldots, d_{h - 1}$. It can be expressed as a recursive function $\Delta_i^m(x)$, with $D_h(x) = \Delta_0^0(x)$:
\begin{equation}
\Delta_i^m(x) = \begin{cases}
    \Delta_{i+1}^m(x) + \hat{W}_i(x) \Delta_{i+1}^{m+2^i}(x) & 0 \leq i \le \log_2(h) \\
    d_m & i = \log_2(h) \\
    \end{cases}
\end{equation}

At each step, the polynomial is split into coefficients whose index has the $i$-th bit set and those which don't. The final steps select the coefficient corresponding to the selected index $m$.

%Because of the properties of $\hat{W}_i$, the vector of evaluations of $\Delta_0^0$ can be computed from two vectors of size $\frac{h}{2}$: the evaluations of $\Delta_1^0$ and $\Delta_1^1$ at even points
Because of the properties of the basis polynomials, the vector of $\frac{h}{2^i}$ evaluations of $\Delta_i^m$ can be computed from two vectors of size $\frac{h}{2^{i + 1}}$: the evaluations of $\Delta_{i+1}^m$ and $\Delta_{i+1}^{m + 2^i}$ at points with the $i + 1$ least significant bits unset.

Let $\Phi(i, m, l) = [\Delta_i^m(\omega_c + \omega_l) \text{ for } c \text{ in } [0, 2^i, \ldots, h - 2^i]]$ be the vector of $\frac{h}{2^i}$ evaluations of $\Delta_i^m$ at all points $\omega_c + \omega_l$ where $c$ has the $i$ most significant bits unset, with $l$ an arbitrary offset.

$\Phi(i, m, l)$ can be computed in $O(n)$ time from $\Phi(i + 1, m, l)$ and $\Phi(i + 1, m + 2^i, l)$.

For each pair of values at index $x$ in the two smaller vectors, the values at indices $2x$ and $2x + 2^i$ in the larger vector can be computed. The values will be denoted as $a, b, a', b'$ for clarity.

$a'$ is straightforwardly computed as:
\begin{equation}a' = \Delta_i^m(\omega_c + \omega_l) = \Delta_{i+1}^m(\omega_c + \omega_l) + \hat{W}_i(\omega_c + \omega_l) \Delta_{i+1}^{m + 2^i}(\omega_c + \omega_l) = a + \hat{W}_i(\omega_c + \omega_l) b\end{equation}

The calculation of $b'$ relies on the properties of the vanishing polynomials:
\begin{equation}b' = \Delta_i^{m}(\omega_c + \omega_l + \omega_{2^i}) = \Delta_{i+1}^m(\omega_c + \omega_l + \omega_{2^i}) + \hat{W}_i(\omega_c + \omega_l + \omega_{2^i}) \Delta_{i+1}^{m + 2^i}(\omega_c + \omega_l + \omega_{2^i})\label{eq:1}\end{equation}

The term $\omega_{2^i}$ vanishes in both $\Delta_{i+1}^m$ and $\Delta_{i+1}^{m + 2^i}$, since both contain only vanishing polynomials $W_j$ with $j \geq i + 1$.

As $\hat{W}_i$ is normalized, $\hat{W}_i(\omega_c + \omega_l + \omega_{2^i}) = \hat{W}_i(\omega_c + \omega_l) + \hat{W}_i(\omega_{2^i}) = \hat{W}_i(\omega_c + \omega_l) + 1$.

\begin{equation}b' = a + (\hat{W}_i(\omega_c + \omega_l) + 1) b = a + \hat{W}_i(\omega_c + \omega_l) b + b = a' + b'\label{eq:1-simplified}\end{equation}

Remember that $a$ and $b$ are the evaluations of $\Delta_{i+1}^m$ and $\Delta_{i+1}^{m + 2^i}$ at $\omega_c + \omega_l$, whereas $a'$ and $b'$ evaluations of $\Delta_i^m$ at $\omega_c + \omega_l$ and $\omega_c + \omega_l + \omega_{2^i}$, respectively.
While the computation of $a'$ is simple and follows from the definition of $\Delta_i^m$, $b'$ is located a different index, and it is only possible to write it in terms of $a$ and $b$ because of the choice of basis polynomials $\hat{W}_i$.

If the standard monomial basis was used, the calculation of $b'$ would require completely different evaluations of $\Delta_{i+1}^m$ and $\Delta_{i+1}^{m + 2^i}$ at the index $\omega_c + \omega_l + \omega_{2^i}$.
There would be no halving of subproblem size at each step of the recursion, so the transform would have complexity $O(n^2)$ instead of $O(n \log n)$.

Computing $a$ and $b$ from $a'$ and $b'$ is similar, and does not require finite field division:
\begin{equation}b' + a' = (a' + b) + a' = b\end{equation}
\begin{equation}a' + \hat{W}_i(\omega_c + \omega_l) b = (a + \hat{W}_i(\omega_c + \omega_l) b) + \hat{W}_i(\omega_c + \omega_l) b = a\end{equation}

Since the individual steps of the forward transform are reversible, the entire transform can be run backwards to obtain the inverse transform.

The vectors can be stored interleaved in a single array, initialized to $[d_0, d_1, \ldots, d_{h - 1}]$ ($h$ single-element vectors), and then updated in-place in $log_2(h)$ steps, each step requiring $O(n)$ time.

See the butterfly diagram in \cite{novel-poly} for a visual representation of the transforms.

In total, $n - 1$ unique factors are needed - one evaluation of $\hat{W}_{\log_2(n)}$, two of $\hat{W}_{\log_2(n) - 1}$, \ldots, $\frac{n}{2}$ evaluations of $\hat{W}_0$ - which can be computed in $O(n \log n)$ time.

See \ref{appendix:transforms} for pseudocode.

\section{Formal Derivative}

The error correction algorithm cannot directly use the inverse transform for interpolation, as the received data is not at contiguous points, and the number of non-corrupted points is likely not a power of two.

Instead, an algorithm based on the formal derivative is used which can recover the original polynomial from any $k$ intact points regardless of error location.

In all fields, the formal derivative of a polynomial is well-defined and the standard power and product rules apply, despite the concepts of limits and continuity not existing in finite fields.
\begin{gather*}
(f \cdot g)' = f' \cdot g + f \cdot g'\\
(\sum_{i = 0}^{n} a_i x^i)' = \sum_{i = 1}^{n} (i \cdot a_i) x^{i - 1}
\end{gather*}

The multiplication $i \cdot a_i$ between an integer and a field element is defined as repeated addition, which in \GF{n} is either zero or $a_i$, as $a_i + a_i = 0$.
\begin{equation}
i \cdot a_i =
    \begin{cases}
        a_i & i\ \text{odd} \\
        0 & i\ \text{even}
    \end{cases}
\end{equation}

Therefore, the formal derivative of a polynomial $f$ in \GF{n} written in the standard monomial basis is:
\begin{equation}f' = a_1 + a_3 x^2 + a_5 x^4 + \ldots\end{equation}

As the normalized vanishing polynomial $\hat{W}_i$ only has coefficients at power-of-two indices, the derivative will be a constant:
\begin{equation}\hat{W}_i' = \frac{\prod_{j = 1}^{2^i - 1} \omega_j}{W_i(2^i)}\end{equation}

To find the derivative of the basis polynomial $X_i$, which is a product of up to 64 polynomials, the product rule generalized to a product of $n$ polynomials is used:

\begin{equation}
(\prod_{i = 0}^{n} f_i)' = \sum_{j = 0}^{n} f_j' \cdot \prod_{i \neq j} f_j
\end{equation}

Therefore, the derivative of $X_i$ contains $|\text{bits}(i)|$ terms, each of which is a basis polynomial with one bit of $i$ unset, multiplied by the derivative of the vanishing polynomial corresponding to that bit:

\begin{equation}
X_i' = \sum_{b \in \text{bits}(i)} \hat{W}_b' \cdot X_{i - 2^b}
\end{equation}

$X_i'$ only has terms with indices less than $i$, so the derivative can be computed in-place by iterating from the lowest degree to the highest degree coefficients (see \ref{appendix:derivative}).

The time complexity of the formal derivative is $O(n \log n)$, since at each step there are up to $\log_2(n)$ bits in the index, and computing the factors $\hat{W}_i'$ for $i = 0, 1, \ldots, \log_2(n) - 1$ also takes $O(n \log n)$ time.

\section{Polynomial Recovery}

In order to recover the original polynomial using the formal derivative, an error locator polynomial is constructed which vanishes at the points where errors occurred.

Let $\text{ERASURES}$ be the set of indices where an error occurred. As previously mentioned, erasure codes require knowledge of all error locations, which will be obtained using hashes.
\begin{equation}e = \prod_{i \in \text{ERASURES}} (x + \omega_i)\end{equation}

Since $e$ does not depend on the actual values of the data polynomial, its values can be computed and multiplied with the incomplete data polynomial $d$, to zero out all unknown values.

The product rule allows the original polynomial $d$ to be recovered:
\begin{gather}
(e \cdot d)' = e' \cdot d + e \cdot d'\\
(e \cdot d)'(\omega_x) = e'(\omega_x) \cdot d(\omega_x) + 0 \cdot d'(\omega_x)\ \forall\ x \in \text{ERASURES}\\
d(\omega_x) = \frac{(e \cdot d)'(\omega_x)}{e'(\omega_x)}\ \forall\ x \in \text{ERASURES}
\end{gather}

Therefore, the original polynomial is recovered by multiplying $d$ by $e$, applying the inverse transform, taking the formal derivative, applying the forward transform,
and finally dividing by $e'$ at the error locations.

\begin{algorithm}
    \caption{Reed-Solomon Decoding}
    \begin{algorithmic}
        \State $\text{t\_fac} \gets \text{PrecomputeFactors}(\log_2(n), 0)$
        \State $\text{d\_fac} \gets \text{PrecomputeDerivativeFactors}(\log_2(n))$
        \State $\text{d} \gets [d_0, d_1, \ldots, d_{n - 1}]$ \Comment{received data}
        \State $\text{erasures} \gets [i_0, i_1, \ldots, i_k]$ \Comment{indices of errors}
        \State $(e, e') \gets \text{ComputeErrorLocator}(\text{erasures}, \text{t\_fac}, \text{d\_fac})$
        \State $\hat{d} \gets d \cdot e$ \Comment{multiply partially corrupt data by error locator polynomial}
        \State $\hat{d'} \gets \text{ForwardTransform}(\text{FormalDerivative}(\text{InverseTransform}(\hat{d}, \text{t\_fac}), \text{d\_fac}), \text{t\_fac})$
        \For{$i \in \text{erasures}$}
            \State $d[i] \gets \hat{d'}[i] / e'[i]$
        \EndFor
    \end{algorithmic}
\end{algorithm}

For this application, $(e, e')$ can be reused for the entire file, with $e'$ inverted in advance, since a corrupt block causes an erasure at the same index in all the interleaved codes.

A $O(n \log n)$ algorithm for computing the error locator is described in \cite{novel-poly} which uses fast Walsh-Hadamard transforms, however it requires $2^r$ operations where $r$ is the power of the field, so it is not useful for \GF{64}.

Instead, I used a $O(n \log^2 n)$ recursive algorithm (\ref{appendix:locator}) which splits the polynomial into two halves, recursively computes each half, and combines the two results by multiplying in $O(n \log n)$ time using the transforms.
\enlargethispage{\baselineskip}
