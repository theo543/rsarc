\chapter{Implementation}

The implementation is written in Rust, using some third-party libraries for I/O, multithreading, hashing, and progress reporting. No libraries were used for the finite field arithmetic, polynomial operations, or Reed-Solomon codes.
See \ref{appendix:dependencies} for a complete list of the libraries used.

\section{Interleaved Codes}

The data and parity files are not treated as one single Reed-Solomon codeword.
Instead, the data is split into blocks. The blocks are \textit{not} independent Reed-Solomon codes, but rather the codes are split across all blocks, with each block containing one symbol from each code.
In other words, the files are treated as matrices, with the blocks as rows and codes as columns.

This is necessary for several reasons:
\begin{itemize}
    \item Using one large Reed-Solomon code would require reading all data into memory, limiting the maximum size to the available memory, or requiring complicated saving and loading to disk as part of the processing.
    \item The precomputed factors for one large code would be as large as the code itself, requiring large amounts of time and space to compute and store.
    \item Since hashes are used to detect corruption, the file must be split into blocks anyways, limiting the granularity of error correction to the block size.
\end{itemize}

Any error occurring in a block will affect all codes, as deleting a row from a matrix affects one element of each column.
Multiple errors in one block have the same effect as a single error. This is suited to common error patterns, which tend not to be uniformly distributed, but instead are burst errors.

See appendix \ref{appendix:bitmap-errors} for a visual example of how burst errors are easier to correct.

\section{Data Storage}

The parity data and metadata are stored in a separate file, specified by the user.

The metadata consists of the header, which specifies the parameters of the encoding - expected data file size, number of data and parity blocks, and block size - and hashes of all blocks, used to detect corruption.
The hashes also include the first 8 bytes of each block, to allow reassembly if the blocks are somehow scrambled, such as by deletion or insertion of a byte. This is very unlikely to happen, but would cause complete failure without a way to put the blocks back in order.

As the Reed-Solomon codes are split across all blocks, reading and writing the data and parity files has a very inefficient access pattern, as reading one code requires one access to each block.
The blocks are analogous to rows in a matrix, and the codes to columns. Processing a matrix stored in row-major order column-wise is inherently inefficient, since non-contiguous memory access is required.

The system call overhead and seek time can be somewhat mitigated by reading many symbols per block at once, as many as can fit into memory.
This produces a large buffer of interleaved symbols, which can then be processed in memory.

Reducing the length of individual codes by increasing block size improves performance, since it allows reading more symbols at once, and therefore passing through the data file fewer times.
However, there is still a penalty for the non-contiguous access, especially on a hard disk drive.

The worst case scenario is if there is not enough memory to read more than one code at once, since this will require one system call per symbol.
In this case, the only option is to increase the block size, which reduces code length, allowing more symbols to be read at once.
This theoretically reduces the granularity of the error correction, causing a single-byte error to render large amounts of data useless for recovery, but since burst errors are most common, this is acceptable.

The performance could also be improved by splitting a file into small sections which fit into memory, but the sections would be independent and could not be used to repair each other.

Writing is implemented using memory mapped I/O, which is simpler to use, but relies completely on the operating system to batch writes to the disk.

Since the data symbols from each code are read in batches - many codes read from each block per pass through the data file - the writes will naturally be batched as well.
Testing does not show a bottleneck in writing.
If necessary, the batching could be done manually, collecting output symbols in a large buffer and using normal write calls, instead of memory mapped I/O.

The same I/O code is used for both encoding and decoding.
When decoding, system calls are used to read uncorrupted symbols from both files, and memory mapped I/O is used to write recovered symbols to both files.
The architecture could be extended to process an arbitrary number of files, such as multiple parity files, single-file archives combining data and parity, or arbitrary folder structures as data instead of a single file.

\section{Multithreaded Processing}

To process codes in parallel, a multithreaded pipeline is used, consisting of a reader thread, an adapter thread, multiple processor threads, and a writer thread.

The reader thread reads codes into an interleaved buffer as described in the previous section.
The processor threads execute the encoding or decoding algorithm, writing the output symbols into a buffer which is sent to the writer thread.
The writer thread simply copies symbols from received buffers into the output memory maps, allowing the operating system to flush pages to disk asynchronously.

In order to synchronize the threads, channels are used to send messages between them. Heap-allocated buffers are used to store input and output data, moving input data from reader to adapter to processor, and output data from processor to writer.

Used input buffers are returned back to the reader and output buffers returned to the processors using separate return channels.

Filled input buffers are sent by the reader to the adapter, which creates a task message for each code in the buffer and sends it to the processor threads through a shared channel, including an offset which specifies which code to read from the buffer,
and a shared atomic counter which is decremented whenever a processor thread finishes processing a code, so that when every code has been processed, the input buffer is sent back to the reader to be reused.

There are five channels used in total for the following purposes:
\begin{itemize}
    \item Sending filled input buffers from the reader to the adapter, along with the number of codes and the index of the first code.
    \item Sending task messages from the adapter to the processors, containing a reference to the input buffer, a shared atomic counter, the index of the code, offset into the buffer, and number of codes in the buffer.
    \item Sending filled output buffers from the processors to the writer, along with the index of the code.
    \item Returning input buffers to the reader after every code has been processed, which is done by decrementing the shared atomic counter and returning the buffer when it reaches zero.
    \item Returning output buffers to the processors after the output symbols have been copied to the memory maps by the writer.
\end{itemize}

The input buffers are protected by a read-write lock, which allows multiple processors to read codes from the buffer at once, but only one thread - the reader - can write to it at a time.
This is only used to ensure thread-safety, not for synchronization, which is done only using channels and the atomic counters.

The processor threads share the same precomputed factors, which depending on the task are either transform factors at multiple offsets for encoding, or transform factors plus derivative factors and the error locator polynomial for decoding.

\section{Benchmarks}

Several kinds of benchmarks were implemented to measure the performance of various aspects of the implementation - finite field arithmetic, polynomial oversampling and recovery, and the full encoding and decoding process including disk I/O.

\subsection{Polynomial Oversampling and Recovery}

This benchmark measures the performance of the encoding and decoding algorithms, as well as of the precomputation of the necessary transform factors, derivative factors, and error locator polynomial.
It also includes a $O(n^2)$ algorithm from an early version of the project, which uses Newton interpolation and Horner's method, for comparison.

The results show the encoding, decoding, and precomputation algorithms scale approximately linearly with the number of symbols, as expected, with the old algorithm scaling quadratically, becoming unusable for large inputs.

For both encoding and decoding, precomputation is slower than the main algorithm, especially for recovery.
However, even for a small block size of one kilobyte (equal to 128 64-bit symbols), the main algorithm will be executed over a hundred times more, so the performance of the precomputation step is not an issue.

\begin{figure}[!hbt]
\begin{center}
\input{benchmarks_log_poly.pgf}
\end{center}
\caption{Log-scale plot of polynomial oversampling and recovery benchmarks.}
\label{fig:benchmark_log_poly}
\end{figure}

\subsection{Finite Field Arithmetic Benchmark}

The results show that inversion by extended Euclidean algorithm is only slightly faster than inversion by raising to $2^{64 - 2}$.
However, with carry-less multiplication disabled, the extended Euclidean algorithm is much faster, since multiplications become more expensive.
As expected, the Russian Peasant fallback is vastly slower than carry-less multiplication.

\input{arithmetic_benchmark.tex}
\vspace{-1em}

\subsection{End-to-End Benchmark}

The full implementation was benchmarked by encoding a randomly generated file, corrupting it, and then decoding it, with $20\%$ redundancy and a block size of $16$ KB.

To detect when only I/O is being performed, CPU usage over than $15\%$ was considered as indicating that significant computation is being performed, measured at a sample rate of $0.2$ seconds.
This is not a precise measurement, but it suffices to show that the program is spending most of its time not in computation, but in I/O.

\begin{figure}[!hbt]
\begin{center}
\input{end_to_end_benchmark.pgf}
\end{center}
\caption{Log-scale plot of end-to-end benchmarks.}
\label{fig:end_to_end_benchmark}
\end{figure}
